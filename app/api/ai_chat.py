"""
AI ì±„íŒ… API for GPT-style streaming responses
"""
from fastapi import APIRouter, HTTPException, Depends
from sqlalchemy.orm import Session
from typing import Optional
import json
import asyncio
from datetime import datetime

from app.core.database import get_db
# SSE ë§¤ë‹ˆì € ì œê±° - ë‹¨ìˆœí•œ SSE êµ¬í˜„ ì‚¬ìš©

router = APIRouter(prefix="/ai", tags=["AI Chat"])


@router.post("/chat")
async def chat_with_ai(
    question: str,
    client_id: str,
    db: Session = Depends(get_db)
):
    """
    AIì™€ ì±„íŒ… (GPT ìŠ¤íƒ€ì¼ ìŠ¤íŠ¸ë¦¬ë°)
    
    ì‹¤ì œ AI API ì—°ë™ ì˜ˆì‹œ:
    - OpenAI GPT
    - Claude
    - Google Gemini
    """
    try:
        # ğŸ”¥ AI ì‘ë‹µ ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œë¡œëŠ” AI API í˜¸ì¶œ)
        ai_response = await simulate_ai_response(question, client_id)
        return {"message": "AI response streaming started", "client_id": client_id}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


async def simulate_ai_response(question: str, client_id: str):
    """
    AI ì‘ë‹µ ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œ AI API ì—°ë™ ì‹œ êµì²´)
    """
    # SSE ë§¤ë‹ˆì € ì œê±° - ë‹¨ìˆœí•œ SSE êµ¬í˜„ ì‚¬ìš©
    print(f"ğŸ¤– AI ì‹œì‘: {client_id} - {question}")
    
    # ì‹œë®¬ë ˆì´ì…˜ëœ AI ì‘ë‹µ (ì‹¤ì œë¡œëŠ” AI APIì—ì„œ ìŠ¤íŠ¸ë¦¬ë°)
    response_chunks = [
        "ì•ˆë…•í•˜ì„¸ìš”! ",
        "CoFriends AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ",
        f"'{question}'ì— ëŒ€í•´ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ",
        "\n\n",
        "íˆ¬í‘œ ì‹œìŠ¤í…œì— ëŒ€í•œ ì§ˆë¬¸ì´ì‹œêµ°ìš”. ",
        "í˜„ì¬ ì‹¤ì‹œê°„ìœ¼ë¡œ íˆ¬í‘œ í˜„í™©ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ",
        "ì–´ë–¤ ì‹ë‹¹ì´ë‚˜ ë©”ë‰´ì— íˆ¬í‘œí•˜ê³  ì‹¶ìœ¼ì‹ ê°€ìš”? ",
        "\n\n",
        "ì¶”ê°€ë¡œ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”!"
    ]
    
    # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì „ì†¡
    for i, chunk in enumerate(response_chunks):
        # SSE ë§¤ë‹ˆì € ì œê±° - ë‹¨ìˆœí•œ SSE êµ¬í˜„ ì‚¬ìš©
        print(f"ğŸ¤– AI ì‘ë‹µ: {client_id} - {chunk}")
        
        # ì‹¤ì œ AI APIì²˜ëŸ¼ ì§€ì—°ì‹œê°„ ì¶”ê°€
        await asyncio.sleep(0.5)
    
    # SSE ë§¤ë‹ˆì € ì œê±° - ë‹¨ìˆœí•œ SSE êµ¬í˜„ ì‚¬ìš©
    print(f"ğŸ¤– AI ì™„ë£Œ: {client_id}")
    
    print(f"ğŸ¤– AI response completed for {client_id}")


@router.post("/chat/stream")
async def stream_ai_chat(
    question: str,
    client_id: str,
    db: Session = Depends(get_db)
):
    """
    ì‹¤ì‹œê°„ AI ì±„íŒ… ìŠ¤íŠ¸ë¦¬ë° (GPT ìŠ¤íƒ€ì¼)
    """
    try:
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ AI ì‘ë‹µ ì²˜ë¦¬
        asyncio.create_task(simulate_ai_response(question, client_id))
        
        return {
            "message": "AI streaming started",
            "client_id": client_id,
            "question": question
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
